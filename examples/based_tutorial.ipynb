{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c097e23-bc5e-430f-ab8d-6146548c9628",
   "metadata": {},
   "source": [
    "# 2. Quick start and Reproducing the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49402616-e1e7-44bb-99f2-e6a099fdfe0d",
   "metadata": {},
   "source": [
    "First, you need to define your own encoder.  \n",
    "In example.py, we’ve provided a simple encoder for you to start with.  \n",
    "You can also replace it with your own encoder.\n",
    "> ℹ️**NOTE: ** \n",
    "> The function `get_encoder_dim()` must be implemented.\n",
    "> It should take the input feature dimension and simply return the output embedding dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f849314-2fc4-4f73-9787-9e3cb54a91e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class standard_code(nn.Module):\n",
    "\n",
    "    def __init__(self, input_x ) -> None:\n",
    "        super().__init__()\n",
    "        # calculate mean\n",
    "        self.mean = torch.mean(input_x, 0)\n",
    "        # calculate sigma\n",
    "        self.sigma = torch.std(input_x, dim=0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        for i, sigma in enumerate(self.sigma):\n",
    "          row = x.t()[i].clone()\n",
    "          row_mean = row.clone()\n",
    "          row_sigma = row.clone()\n",
    "          row_mean[row_mean>self.mean[i]] = 1\n",
    "          row_mean[row_mean<self.mean[i]] = 0\n",
    "          row_sigma[row_sigma>self.sigma[i]+self.mean[i]] = 1\n",
    "          row_sigma[row_sigma<self.sigma[i]-self.mean[i]] = 1\n",
    "          row_sigma[row_sigma<self.sigma[i]+self.mean[i]] = 0\n",
    "          row_sigma[row_sigma>self.sigma[i]-self.mean[i]] = 0\n",
    "          encode = torch.stack((row_mean,row_sigma),1)\n",
    "          if ( i == 0 ):\n",
    "            output = encode\n",
    "          else:\n",
    "            output = torch.cat( ( output, encode ), 1 )\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_encoder_dim(self, input_dim):\n",
    "        output_dim = input_dim*2\n",
    "        return output_dim\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d214e1-2600-44b3-a965-0a6fa7207e94",
   "metadata": {},
   "source": [
    "Now you can run the code!  \n",
    "In example.py, simply call SCARF_Experiment() and pass in the encoder you just defined.  \n",
    "Then, choose the dataset you want to experiment with and whether to apply reshaping.  \n",
    "\n",
    "There are four dataset options:  \n",
    "\n",
    "* `all`\n",
    "\n",
    "* `binary`\n",
    "\n",
    "* `multi`\n",
    "\n",
    "* `high dim`\n",
    "\n",
    "* `large`\n",
    "\n",
    "For detailed descriptions of each dataset category, please refer to Appendix B of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aa1de8-5924-4cea-b745-93fdc2784d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = scarf_exp.SCARF_Experiment(standard_code, \"large\", encoder_reshape=True )\n",
    "exp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451ae2fc-6c6d-4716-a76d-99561fc212f6",
   "metadata": {},
   "source": [
    "The above describes how to replace the encoder from the original paper.  \n",
    "If you just want to reproduce the experiment, you only need to modify the contents of the `config.yaml` and select the desired dataset — there’s no need to define an encoder manually.  \n",
    "\n",
    "A sample config.yaml is provided ...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
